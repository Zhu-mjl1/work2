{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d4664a",
   "metadata": {},
   "source": [
    "# WEEK 14\n",
    "\n",
    "2024/07/08 - 2024/07/14\n",
    "\n",
    "## 深度学习 C4_W4\n",
    "\n",
    "### 1. 实施神经风格转移算法\n",
    "\n",
    "- **神经风格转移算法**：\n",
    "  - 将内容图像和风格图像合成为一幅既包含内容图像的结构又具有风格图像的艺术效果的新图像。\n",
    "  - 主要利用卷积神经网络（CNN）提取图像的内容特征和风格特征，通过优化技术使得生成图像同时满足内容和风格的要求。\n",
    "\n",
    "### 2. 定义神经风格转换的内容成本函数\n",
    "\n",
    "- **内容成本函数**：\n",
    "  - 目的是保持生成图像与内容图像的相似性。\n",
    "  - 计算内容图像与生成图像在选定卷积层输出的特征图上的差异。\n",
    "\n",
    "- **公式**：\n",
    "$$\n",
    "  L_{content}(C, G) = \\sum_{i, j} (F_{ij}^C - F_{ij}^G)^2\n",
    "$$\n",
    "  其中 $ F_{ij}^C $ 和 $ F_{ij}^G $ 分别是内容图像和生成图像在选定层的特征图。\n",
    "\n",
    "### 3. 定义神经风格转换的风格成本函数\n",
    "\n",
    "- **风格成本函数**：\n",
    "  - 目的是使生成图像的风格与风格图像的风格相匹配。\n",
    "  - 使用Gram矩阵来表示图像的风格，计算生成图像和风格图像的Gram矩阵之间的差异。\n",
    "\n",
    "- **公式**：\n",
    "$$\n",
    "  L_{style}(S, G) = \\sum_{l=0}^L \\frac{1}{4N_l^2 M_l^2} \\sum_{i, j} (G_{ij}^S - G_{ij}^G)^2\n",
    "  $$\n",
    "  其中 $ G_{ij}^S $ 和 $G_{ij}^G $ 分别是风格图像和生成图像在第 $ l $ 层的Gram矩阵。\n",
    "\n",
    "### 4. 利用神经风格转移生成新颖的艺术图像\n",
    "\n",
    "- **实现步骤**：\n",
    "  1. **加载预训练模型**：使用VGG19模型提取内容和风格特征。\n",
    "  2. **定义成本函数**：包括内容成本函数和风格成本函数。\n",
    "  3. **优化生成图像**：通过优化技术调整生成图像，使其同时满足内容和风格的要求。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# 加载预训练的VGG19模型\n",
    "vgg = VGG19(include_top=False, weights='imagenet')\n",
    "vgg.trainable = False\n",
    "\n",
    "# 内容和风格图像路径\n",
    "content_image_path = 'path/to/content/image.jpg'\n",
    "style_image_path = 'path/to/style/image.jpg'\n",
    "\n",
    "# 加载和预处理图像\n",
    "def load_and_process_image(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "content_image = load_and_process_image(content_image_path)\n",
    "style_image = load_and_process_image(style_image_path)\n",
    "\n",
    "# 提取内容和风格特征\n",
    "def get_feature_representations(model, content_image, style_image):\n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    content_output = [model.get_layer(name).output for name in content_layers]\n",
    "    style_output = [model.get_layer(name).output for name in style_layers]\n",
    "    model_outputs = content_output + style_output\n",
    "    multi_output_model = Model(model.input, model_outputs)\n",
    "    content_features = multi_output_model(content_image)\n",
    "    style_features = multi_output_model(style_image)\n",
    "    return content_features, style_features\n",
    "\n",
    "content_features, style_features = get_feature_representations(vgg, content_image, style_image)\n",
    "\n",
    "# 定义内容和风格损失函数\n",
    "def compute_content_loss(content, generated):\n",
    "    return tf.reduce_mean(tf.square(content - generated))\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', tensor, tensor)\n",
    "    input_shape = tf.shape(tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result / num_locations\n",
    "\n",
    "def compute_style_loss(style, generated):\n",
    "    S = gram_matrix(style)\n",
    "    G = gram_matrix(generated)\n",
    "    return tf.reduce_mean(tf.square(S - G))\n",
    "\n",
    "# 创建损失函数\n",
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    model_outputs = model(init_image)\n",
    "    content_output_features = model_outputs[:len(content_layers)]\n",
    "    style_output_features = model_outputs[len(content_layers):]\n",
    "\n",
    "    content_score = 0\n",
    "    style_score = 0\n",
    "\n",
    "    weight_content, weight_style = loss_weights\n",
    "\n",
    "    for target_content, comb_content in zip(content_features, comb_content_features):\n",
    "        content_score += compute_content_loss(target_content, comb_content)\n",
    "\n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        style_score += compute_style_loss(target_style, comb_style)\n",
    "\n",
    "    content_score *= weight_content\n",
    "    style_score *= weight_style\n",
    "    loss = content_score + style_score\n",
    "    return loss\n",
    "\n",
    "# 进行优化\n",
    "generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.02)\n",
    "\n",
    "# 超参数\n",
    "epochs = 1000\n",
    "content_weight = 1e3\n",
    "style_weight = 1e-2\n",
    "loss_weights = (content_weight, style_weight)\n",
    "\n",
    "# 训练模型\n",
    "@tf.function()\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(vgg, loss_weights, generated_image, style_features, content_features)\n",
    "    total_loss = all_loss\n",
    "    grads = tape.gradient(total_loss, generated_image)\n",
    "    opt.apply_gradients([(grads, generated_image)])\n",
    "    generated_image.assign(tf.clip_by_value(generated_image, -103.939, 151.061))\n",
    "    return total_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step(generated_image)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "```\n",
    "\n",
    "### 5. 神经风格转移的应用\n",
    "\n",
    "- **创意产业**：将普通照片转换为艺术风格图像，应用于广告、艺术创作和影视制作。\n",
    "- **个人用户**：生成个性化艺术图像，作为社交媒体内容。\n",
    "\n",
    "## 深度学习 C4_W3\n",
    "\n",
    "### 1. 识别用于物体检测的组件\n",
    "\n",
    "- **物体检测的关键组件**：\n",
    "  - **边界框（Bounding Box）**：用于框出图像中的目标物体。\n",
    "  - **特征提取器（Feature Extractor）**：用于提取图像中的特征信息。\n",
    "  - **分类器（Classifier）**：用于对检测到的物体进行分类。\n",
    "\n",
    "### 2. 实施物体检测\n",
    "\n",
    "- **常见物体检测算法**：\n",
    "  - **YOLO（You Only Look Once）**：一种单次检测算法，能够在一次前向传播中同时预测多个物体的边界框和类别。\n",
    "  - **SSD（Single Shot MultiBox Detector）**：类似于YOLO，使用多尺度特征图进行物体检测。\n",
    "  - **Faster R-CNN**：基于区域提案网络（RPN）的两阶段检测算法，具有较高的检测精度。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output_tensor = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    return model\n",
    "\n",
    "model = build_model((128, 128, 3))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### 3. 实施非最大压制以提高准确性\n",
    "\n",
    "- **非最大压制（Non-Maximum Suppression, NMS）**：\n",
    "  - 用于去除冗余的边界框，只保留最有可能包含物体的边界框。\n",
    "  - 通过计算每个边界框的置信度分数，删除与最高分边界框重叠且置信度较低的其他边界框。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "def non_maximum_suppression(boxes, scores, iou_threshold):\n",
    "    indices = tf.image.non_max_suppression(boxes, scores, max_output_size=10, iou_threshold=iou_threshold)\n",
    "    selected_boxes = tf.gather(boxes, indices)\n",
    "    selected_scores = tf.gather(scores, indices)\n",
    "    return selected_boxes, selected_scores\n",
    "\n",
    "boxes = tf.constant([[0.1, 0.1, 0.5, 0.5], [0.2, 0.2, 0.6, 0.6], [0.7, 0.7, 0.8, 0.8]])\n",
    "scores = tf.constant([0.9, 0.75, 0.6])\n",
    "selected_boxes, selected_scores = non_maximum_suppression(boxes, scores, iou_threshold=0.5)\n",
    "print(\"Selected boxes:\", selected_boxes.numpy())\n",
    "print(\"Selected scores:\", selected_scores.numpy())\n",
    "```\n",
    "\n",
    "### 4. 实现相交重于联合处理边界\n",
    "\n",
    "- **相交重于联合（Intersection over Union, IoU）**：\n",
    "  - 用于衡量两个边界框之间的重叠程度。\n",
    "  - 计算公式：\\[ \\text{IoU} = \\frac{\\text{交集面积}}{\\text{并集面积}} \\]\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "def iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    return intersection / union\n",
    "\n",
    "box1 = [0.1, 0.1, 0.5, 0.5]\n",
    "box2 = [0.2, 0.2, 0.6, 0.6]\n",
    "print(\"IoU:\", iou(box1, box2))\n",
    "```\n",
    "\n",
    "### 5. 应用稀疏分类交叉熵进行像素预测\n",
    "\n",
    "- **稀疏分类交叉熵**：\n",
    "  - 用于多类分类问题，特别适用于类别较多但每个样本只属于一个类别的情况。\n",
    "  - 计算公式与交叉熵类似，但不需要将标签转换为独热编码。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义模型\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6e5d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
