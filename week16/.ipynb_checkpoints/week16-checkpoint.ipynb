{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9739f78f",
   "metadata": {},
   "source": [
    "---\n",
    "# WEEK 16\n",
    "\n",
    "2024/07/22 - 2024/07/28\n",
    "\n",
    "## 深度学习 C5_W2\n",
    "\n",
    "### 1. 词语嵌入\n",
    "\n",
    "- **词语嵌入**：\n",
    "  - 是一种将词汇表示为低维向量的技术，捕捉词汇之间的语义关系。\n",
    "  - 常用的词嵌入方法包括Word2Vec、GloVe等。\n",
    "\n",
    "- **词向量**：\n",
    "  - 将词汇表示为连续的实数向量。\n",
    "  - 向量的维度通常较低，以减少计算复杂度。\n",
    "  - 相似词汇在向量空间中的距离较近。\n",
    "\n",
    "### 2. 词向量之间的相似度\n",
    "\n",
    "- **余弦相似度**：\n",
    "  - 计算两个向量之间的余弦值，范围在-1到1之间。\n",
    "  - 相似度公式：\\[ \\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} \\]\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "```\n",
    "\n",
    "### 3. 使用单词嵌入解决单词类比问题\n",
    "\n",
    "- **单词类比问题**：\n",
    "  - 给定词对(A:B)和一个词C，找到D使得C:D与A:B的关系相似。\n",
    "  - 例如，“男人:女人”与“国王:王后”的类比问题。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "def find_analogy(word_a, word_b, word_c, word_vectors):\n",
    "    vec_a = word_vectors[word_a]\n",
    "    vec_b = word_vectors[word_b]\n",
    "    vec_c = word_vectors[word_c]\n",
    "    \n",
    "    analogy_vec = vec_b - vec_a + vec_c\n",
    "    similarities = {word: cosine_similarity(analogy_vec, vec) for word, vec in word_vectors.items()}\n",
    "    most_similar = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return most_similar[0][0]\n",
    "\n",
    "# 示例使用预训练的词向量\n",
    "word_vectors = {'man': np.array([0.2, 0.4]), 'woman': np.array([0.3, 0.5]), 'king': np.array([0.5, 0.7]), 'queen': np.array([0.6, 0.8])}\n",
    "result = find_analogy('man', 'woman', 'king', word_vectors)\n",
    "print(result)  # 输出 'queen'\n",
    "```\n",
    "\n",
    "### 4. 减少词语嵌入中的偏差\n",
    "\n",
    "- **偏差问题**：\n",
    "  - 词向量可能捕捉到数据中的偏见（例如性别、种族等）。\n",
    "  - 解决方法包括对向量进行去偏处理，确保向量表示的公平性。\n",
    "\n",
    "### 5. 在 Keras 中使用预训练词向量创建嵌入层\n",
    "\n",
    "- **嵌入层**：\n",
    "  - Keras中的嵌入层用于将词汇映射到其对应的词向量。\n",
    "  - 可以使用预训练的词向量（例如GloVe）来初始化嵌入层。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# 示例预训练词向量（实际应用中应加载真实的预训练词向量）\n",
    "embedding_matrix = np.array([[0.2, 0.4], [0.3, 0.5], [0.5, 0.7], [0.6, 0.8]])\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=4, output_dim=2, weights=[embedding_matrix], trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# 示例输入\n",
    "input_data = np.array([[0, 1, 2], [1, 2, 3]])\n",
    "output = model.predict(input_data)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 6. 负抽样学习词向量\n",
    "\n",
    "- **负抽样（Negative Sampling）**：\n",
    "  - 是一种用于训练词向量的技术，通过将预测目标从整个词汇表中的一个词缩小到一个小的负采样集，显著减少计算量。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 假设词汇表和对应的词向量\n",
    "vocabulary_size = 10000\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.random.rand(vocabulary_size, embedding_dim)\n",
    "\n",
    "# 创建负采样训练数据\n",
    "def generate_negative_samples(target_word, context_words, num_negative_samples=5):\n",
    "    negative_samples = []\n",
    "    for context_word in context_words:\n",
    "        negative_samples.extend(np.random.choice(vocabulary_size, num_negative_samples, replace=False))\n",
    "    return negative_samples\n",
    "\n",
    "# 负采样训练函数\n",
    "def train_with_negative_sampling(target_word, context_words, embedding_matrix, learning_rate=0.01):\n",
    "    target_vector = embedding_matrix[target_word]\n",
    "    context_vectors = embedding_matrix[context_words]\n",
    "    negative_samples = generate_negative_samples(target_word, context_words)\n",
    "    negative_vectors = embedding_matrix[negative_samples]\n",
    "\n",
    "    positive_loss = -np.log(sigmoid(np.dot(context_vectors, target_vector)))\n",
    "    negative_loss = -np.log(sigmoid(-np.dot(negative_vectors, target_vector)))\n",
    "\n",
    "    total_loss = positive_loss + np.sum(negative_loss)\n",
    "    gradient = total_loss * learning_rate\n",
    "\n",
    "    embedding_matrix[target_word] -= gradient\n",
    "    embedding_matrix[context_words] -= gradient\n",
    "    embedding_matrix[negative_samples] -= gradient\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "# Sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "### 7. GloVe 算法\n",
    "\n",
    "- **GloVe（Global Vectors for Word Representation）**：\n",
    "  - 是一种基于词共现矩阵的词向量训练方法。\n",
    "  - 通过构建词对共现矩阵，优化损失函数来学习词向量。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def build_cooccurrence_matrix(corpus, vocabulary_size, window_size=5):\n",
    "    cooccurrence_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
    "    for text in corpus:\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            word_index = word_to_index[word]\n",
    "            context_indices = range(max(0, i - window_size), min(len(words), i + window_size + 1))\n",
    "            for j in context_indices:\n",
    "                if i != j:\n",
    "                    context_word_index = word_to_index[words[j]]\n",
    "                    cooccurrence_matrix[word_index, context_word_index] += 1\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "def train_glove(cooccurrence_matrix, embedding_dim=300, iterations=100, learning_rate=0.01):\n",
    "    vocabulary_size = cooccurrence_matrix.shape[0]\n",
    "    W = np.random.rand(vocabulary_size, embedding_dim)\n",
    "    b = np.random.rand(vocabulary_size)\n",
    "    for _ in range(iterations):\n",
    "        for i in range(vocabulary_size):\n",
    "            for j in range(vocabulary_size):\n",
    "                if cooccurrence_matrix[i, j] > 0:\n",
    "                    x_ij = cooccurrence_matrix[i, j]\n",
    "                    w_i = W[i]\n",
    "                    w_j = W[j]\n",
    "                    b_i = b[i]\n",
    "                    b_j = b[j]\n",
    "                    f_x = (w_i @ w_j) + b_i + b_j - np.log(x_ij)\n",
    "                    gradient = f_x * learning_rate\n",
    "                    W[i] -= gradient\n",
    "                    W[j] -= gradient\n",
    "                    b[i] -= gradient\n",
    "                    b[j] -= gradient\n",
    "    return W\n",
    "```\n",
    "\n",
    "### 8. 词嵌入构建情感分类器\n",
    "\n",
    "- **情感分类器**：\n",
    "  - 使用词嵌入作为输入，训练神经网络模型进行文本情感分类。\n",
    "  - 常用的网络结构包括LSTM、双向LSTM等。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 示例词嵌入矩阵（实际应用中应加载真实的预训练词向量）\n",
    "embedding_matrix = np.random.rand(10000, 300)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=300, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 示例数据\n",
    "input_data = np.random.randint(10000, size=(1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "model.fit(input_data, labels, epochs=10)\n",
    "```\n",
    "\n",
    "### 9. LSTM 建立并训练复杂分类器\n",
    "\n",
    "- **LSTM（长短期记忆网络）**：\n",
    "  - 是一种改进的RNN，能够更好地捕捉长期依赖关系，缓解梯度消失问题。\n",
    "  - 适用于处理序列数据，如文本、时间序列等。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(100, 300)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 示例数据\n",
    "input_data = np.random.rand(1000, 100, 300)\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "model.fit(input_data, labels, epochs=10)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
