{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62451b31",
   "metadata": {},
   "source": [
    "# WEEK 18\n",
    "\n",
    "2024/08/05 - 2024/08/11\n",
    "\n",
    "## 深度学习 C5_W4\n",
    "\n",
    "### 1. 创建位置编码\n",
    "\n",
    "- **位置编码（Positional Encoding）**：\n",
    "  - 在Transformer模型中，没有卷积或递归结构来捕捉输入序列中的顺序信息，因此需要为输入序列添加位置编码。\n",
    "  - 位置编码通过正弦和余弦函数为每个位置生成唯一的向量，并将其添加到输入嵌入中。\n",
    "  - 位置编码的公式如下：\n",
    "    \\[\n",
    "    PE_{(pos, 2i)} = \\sin \\left( \\frac{pos}{10000^{2i/d_{model}}} \\right)\n",
    "    \\]\n",
    "    \\[\n",
    "    PE_{(pos, 2i+1)} = \\cos \\left( \\frac{pos}{10000^{2i/d_{model}}} \\right)\n",
    "    \\]\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# 示例\n",
    "position = 50\n",
    "d_model = 512\n",
    "pos_encoding = positional_encoding(position, d_model)\n",
    "print(pos_encoding.shape)  # (1, 50, 512)\n",
    "```\n",
    "\n",
    "### 2. 捕捉数据中的顺序关系\n",
    "\n",
    "- **顺序关系**：\n",
    "  - 通过位置编码，Transformer能够捕捉输入序列中的顺序关系。\n",
    "  - 位置编码为每个时间步的输入增加了位置信息，使模型能够区分相同词语在不同位置的不同含义。\n",
    "\n",
    "### 3. 利用词嵌入计算缩放点积自注意力\n",
    "\n",
    "- **缩放点积自注意力（Scaled Dot-Product Attention）**：\n",
    "  - 自注意力机制用于计算序列中每个元素对其他元素的注意力权重。\n",
    "  - 缩放点积自注意力的计算公式如下：\n",
    "    \\[\n",
    "    Attention(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "    \\]\n",
    "  - 其中，\\(Q\\)、\\(K\\)和\\(V\\)分别是查询、键和值矩阵，\\(d_k\\)是键向量的维度。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# 示例\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "q = tf.random.uniform((1, 60, d_model))\n",
    "k = tf.random.uniform((1, 60, d_model))\n",
    "v = tf.random.uniform((1, 60, d_model))\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, None)\n",
    "print(output.shape)  # (1, 60, 512)\n",
    "```\n",
    "\n",
    "### 4. 实施遮蔽式多头关注\n",
    "\n",
    "- **多头注意力（Multi-Head Attention）**：\n",
    "  - 在多头注意力中，使用多个注意力头并行处理输入，以捕捉不同位置之间的关系。\n",
    "  - 每个注意力头都有自己的查询、键和值矩阵，然后将这些头的输出拼接在一起，并通过一个线性层进行变换。\n",
    "\n",
    "- **遮蔽（Masking）**：\n",
    "  - 遮蔽用于避免在计算注意力时关注到未来的词语或填充的词语。\n",
    "  - 在训练时，使用前向遮蔽（look-ahead mask）来防止模型看到未来的词语。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "```\n",
    "\n",
    "### 5. 建立并训练变压器模型\n",
    "\n",
    "- **变压器模型（Transformer Model）**：\n",
    "  - 变压器模型由编码器和解码器组成，每个编码器和解码器都有多个层堆叠而成。\n",
    "  - 编码器层包括多头注意力和前馈神经网络，解码器层还包括额外的多头注意力用于接收编码器的输出。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# 示例\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "dropout_rate = 0.1\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "sample_encoder_output = encoder_layer(tf.random.uniform((64, 43, d_model)), False, None)\n",
    "print(sample_encoder_output.shape)  # (64, 43, 512)\n",
    "```\n",
    "\n",
    "### 6. 微调用于命名实体识别的预训练转换器模型\n",
    "\n",
    "- **预训练转换器模型（Pre-trained Transformer Models）**：\n",
    "  - 预训练的Transformer模型如BERT、GPT-3等，已在大规模语料库上进行了预训练，可以用于各种下游任务。\n",
    "  - 可以对预训练模型进行微调，以适应特定任务，如命名实体识别（NER）。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "input_ids = tf.constant(tokenizer.encode(\"Hello, my name is Bert.\", add_special_tokens=True))[None, :]\n",
    "outputs = bert_model(input_ids)\n",
    "\n",
    "print(outputs.last_hidden_state.shape)  # (1, 8, 768)\n",
    "```\n",
    "\n",
    "### 7. 使用预训练模型进行命\n",
    "\n",
    "名实体识别\n",
    "\n",
    "- **命名实体识别（NER）**：\n",
    "  - NER是从文本中识别出实体名称（如人名、地名、组织名等）的任务。\n",
    "  - 可以使用预训练的BERT模型进行微调，使其适应NER任务。\n",
    "\n",
    "- **代码示例**：\n",
    "\n",
    "```python\n",
    "from transformers import TFBertForTokenClassification\n",
    "\n",
    "ner_model = TFBertForTokenClassification.from_pretrained('bert-base-cased', num_labels=9)\n",
    "ner_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 示例数据\n",
    "X = tf.random.uniform((32, 50), maxval=100, dtype=tf.int32)\n",
    "y = tf.random.uniform((32, 50), maxval=9, dtype=tf.int32)\n",
    "\n",
    "ner_model.fit(X, y, epochs=3)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
