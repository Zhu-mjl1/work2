# Notes: Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring

## 1. 引言

### 1.1 背景
1. 随着图像-文本预训练模型（如CLIP）的发展，这类模型展示了其通过大规模图像-文本配对数据所学到的多模态知识，能够在多种图像理解任务中表现出色。
2. 然而，视频任务（如视频-文本检索、视频识别等）不仅需要空间特征的提取，还需要有效的时序建模来捕捉不同帧之间的时间依赖关系。
3. 获取类似CLIP规模的预训练视频-文本数据集代价极高，因此直接迁移图像模型到视频任务中成为一种有效的解决方案。
4. 视频任务对模型提出了更高的要求，尤其在如何有效利用图像预训练模型捕捉视频中的时序信息方面存在挑战。

### 1.2 问题定义
1. 当前的时序建模方法通常针对两类任务之一：高层语义任务（如视频-文本检索）或低层视觉模式任务（如视频识别）。然而，单一的方法难以同时适用于这两类任务。
2. CLIP模型在预训练时学到了高层语义知识（如图像与文本的对齐信息）和低层视觉特征（如空间结构和模式），如何在视频任务中利用这两类知识是一个关键问题。

### 1.3 研究目标
1. 文章提出了一种新的时序建模机制——**空间-时间辅助网络（STAN）**，用于将CLIP模型扩展到视频任务中，同时利用其高层语义知识和低层视觉特征。
2. STAN通过引入分支结构，在不影响CLIP原有网络结构的前提下，增强其对视频时序信息的捕捉能力，以应对视频任务中的多样化需求。

## 2. 相关工作

### 2.1 视觉-语言预训练模型
1. CLIP是一种图像-文本对齐模型，预训练于4亿对图像-文本数据上，在图像和文本的多模态对齐任务中取得了巨大的成功。
2. 类似CLIP的视觉-语言模型展示了其在多个下游任务中的迁移能力，尤其是在无监督领域中，能够执行零样本分类和领域泛化任务。
3. 对于视频任务，虽然存在直接的视频-文本预训练模型，但由于数据采集困难和计算资源限制，这类模型难以达到CLIP在图像-文本领域的规模和表现。

### 2.2 时序建模方法
1. **后置结构方法**：后置结构方法将CLIP模型视为图像特征提取器，分别提取每一帧的特征后，再进行时序建模。该方法保留了CLIP模型的高层语义信息（如视觉-语言对齐能力），但在捕捉帧间时序信息上表现不佳，尤其是在动态模式任务中效果有限。
2. **中间结构方法**：中间结构通过在CLIP模型的不同层之间插入时序建模模块（如3D卷积或自注意力），提高了其对视频时序信息的捕捉能力。然而，这种方法在损害CLIP高层语义知识的同时，仅适用于低层视觉模式任务。
3. **问题总结**：现有的时序建模方法未能有效兼顾CLIP模型的多层次知识，无法同时满足视频检索和视频识别等任务的需求。

## 3. 方法论

### 3.1 STAN概述
1. **动机**：STAN的设计目的是为了解决现有时序建模方法在高层语义任务与低层视觉任务间难以兼顾的问题。通过设计分支结构，STAN在不干扰CLIP原有视觉编码的情况下增强其时序建模能力。
2. **基本思路**：STAN采用一个分支结构，该结构并行于CLIP的视觉骨干网络，通过多层次的空间和时序建模机制，能够在视频任务中实现更加精确的表示学习。

### 3.2 STAN的分支结构设计
1. STAN的核心是一个分支结构，这一分支与CLIP的视觉编码器并行工作。在该分支结构中，STAN通过一系列的空间-时间分解模块对CLIP模型的多层特征进行处理。
2. 每一层STAN模块都由两个子模块组成：
   - **帧内模块**：用于在每一帧图像中进行空间建模，捕捉帧内特征的空间关系。
   - **跨帧模块**：用于在不同帧之间进行时序建模，捕捉帧间的时间依赖关系。
3. STAN允许在CLIP的多个层次上执行空间-时间上下文的学习，使得视频帧的特征能够在时序和空间上得到充分利用。

### 3.3 时序建模的具体实现
1. **自注意力模块**：自注意力机制通过对同一空间位置上的所有帧进行全局建模，从而捕捉帧间的时间依赖。这种方式能够较好地处理长距离时序依赖信息。
2. **3D卷积模块**：3D卷积是一种较为传统的时序建模方式，通过在时间维度上进行卷积操作来捕捉局部的时序模式。3D卷积更适合局部时序信息的建模，且计算效率较高。
3. **跨帧信息融合**：最终，STAN会融合所有层次的时序信息，使得输出的视频特征既包含空间信息，又包含时间依赖。

## 4. 实验

### 4.1 实验设置
1. **数据集**：
   - **MSR-VTT**：一个用于视频-文本检索的大规模数据集，包含10,000个YouTube视频，每个视频有20个文本描述。
   - **DiDeMo**：另一个视频-文本检索数据集，包含10,000个视频和40,000个文本描述，视频持续时间较长。
   - **Kinetics-400**：一个广泛使用的视频动作识别数据集，包含260,000个视频和400个动作类别。
   - **Something-Something-V2**：专注于时序建模的视频识别数据集，包含220,485个视频和174个动作类别。
2. **训练设置**：
   - 不同数据集的帧数量和输入token长度根据任务的不同有所调整。例如在MSR-VTT和LSMDC中使用12帧，而在DiDeMo中使用64帧。
   - 采用Adam优化器，学习率分别设置为2e-6（CLIP部分）和2e-5（STAN部分），并采用余弦退火调度策略进行学习率的调整。

### 4.2 实验结果对比
1. **视频-文本检索任务**：
   - 在MSR-VTT、DiDeMo和LSMDC这三个数据集上，STAN模型相比现有的SOTA方法（如CLIP4Clip）取得了显著的性能提升。
   - STAN模型通过有效的时序建模机制，显著提升了视频-文本检索任务中的检索精度，尤其在高层语义任务中表现突出。
2. **视频识别任务**：
   - 在Kinetics-400和Something-Something-V2数据集上，STAN在视频识别任务中的表现同样优异。在Kinetics-400中，STAN在保持较低计算成本的前提下，超过了许多更大规模的模型（如Swin3D-L）。
   - 在SSv2数据集上，STAN模型通过对时序信息的有效建模，取得了超过20%的准确率提升，展示了其在时序敏感任务中的强大能力。

### 4.3 消融实验
1. **分支结构贡献**：在实验中，移除STAN的分支结构会显著降低模型在视频任务中的表现，这表明分支结构对于多层次时序信息的捕捉至关重要。
2. **时序模块贡献**：STAN的时序建模模块是性能提升的关键。通过消融实验发现，单独的空间或时间模块都不足以带来显著的性能提升，而空间和时序建模的结合能够显著提升模型的表现。

## 5. 讨论

### 5.1 知识迁移与时序建模
1. STAN通过分支结构有效地利用了CLIP的多层次特征，同时捕捉了视频任务中的时序信息，避免了对CLIP高层语义知识的损害。
2. 传统时序建模方法在增强低层视觉模式时，往往

破坏了CLIP模型中原有的视觉-语言对齐知识，而STAN能够在保持高层语义知识的同时，增强其在时序任务中的表现。

### 5.2 STAN的广泛适用性
1. 在视频-文本检索任务中，STAN模型通过更加精确的时序建模，增强了视频与文本的对齐能力，使其在复杂场景中表现更加稳定。
2. 在视频识别任务中，STAN的时序建模能力得到了进一步验证，尤其在需要处理动态时序依赖的任务中，STAN的表现优于传统的基于卷积的模型。

## 6. 结论
1. 文章重新审视了基于CLIP的图像到视频知识迁移中的时序建模问题，提出了空间-时间辅助网络（STAN），该方法通过分支结构同时捕捉高层语义和低层视觉信息，成功应用于多种视频任务中。
2. STAN模型在视频-文本检索和视频识别任务中均超过了现有的最先进方法，展示了其在不同任务中的广泛适用性。
3. 未来研究可以进一步优化STAN的计算效率，探讨如何更好地利用CLIP模型在更大范围的视频任务中的潜力。

