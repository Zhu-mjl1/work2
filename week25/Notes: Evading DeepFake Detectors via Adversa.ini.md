# Evading DeepFake Detectors via Adversarial Statistical Consistency 
## 1. 引言
随着DeepFake技术的迅速进步，基于深度生成模型（如生成对抗网络，GAN）生成的逼真伪造人脸图像和视频变得越来越普遍。尽管人类很难分辨这些伪造内容，现有的DeepFake检测器依赖于在空间域和频率域中检测自然图像和DeepFake图像之间的统计差异来有效识别。然而，检测器容易受到对抗性攻击的威胁，即通过在伪造图像上注入微小的扰动来逃避检测。

文章提出了一种新的统计一致性攻击（StatAttack），其主要通过减少伪造图像和自然图像之间的统计差异来逃避DeepFake检测器。该方法包含两个核心部分：第一，选择统计敏感的自然退化（如曝光、模糊和噪声）并对伪造图像进行对抗性添加；第二，提出分布感知损失函数来优化这些退化，使得生成的伪造图像特征分布更接近自然图像。此外，文章还提出了扩展版本MStatAttack，通过多层退化实现更强的攻击效果。

### 主要贡献：
1. 提出一种基于自然退化的对抗攻击方法StatAttack，能够有效减少伪造图像和自然图像的分布差异。
2. 提出多层次对抗攻击方法MStatAttack，通过优化多层退化组合，生成更自然的对抗样本。
3. 在四个空间域检测器和两个频率域检测器上进行了实验，验证了方法在白盒和黑盒攻击环境中的有效性。

## 2. 相关工作
### 2.1 DeepFake生成
DeepFake技术主要利用生成对抗网络（GAN）生成虚假图像和视频，这些技术分为以下几类：
- **整脸合成**：例如ProGAN和StyleGAN技术用于生成不存在的逼真脸部图像。
- **身份替换**：如FaceSwap工具，通过将一个人的脸替换为另一个目标人物的脸来实现身份伪造。
- **面部操纵**：如StarGAN，通过改变面部属性（如发色、年龄等）对人脸进行操纵。

这些DeepFake生成方法在政治、娱乐、媒体等领域的滥用引发了广泛的关注，特别是在隐私与信息安全方面的潜在风险。

### 2.2 DeepFake检测
DeepFake检测器旨在通过深度学习模型从图像中提取特征来区分伪造图像与自然图像。检测器主要依赖于以下两类信息：
- **空间域信息**：一些检测方法通过分析图像的亮度、纹理等空间域信息来识别伪造图像。
- **频率域信息**：其他方法通过识别图像频率成分（如高频噪声）的差异来检测伪造图像。

### 2.3 对抗攻击
对抗攻击的目标是生成伪造样本，这些样本经过添加微小扰动后能够欺骗模型，导致模型输出错误结果。已有研究表明，对抗样本可以在白盒攻击中成功欺骗模型，但迁移到不同检测器时的效果较差。文章的目标是设计出能够具有良好迁移性的对抗攻击方法，使其能够同时骗过不同的DeepFake检测器。

## 3. 统计差异
在自然图像与伪造图像之间，统计差异主要体现在亮度和频率上：
- **亮度统计差异**：McCloskey等人发现，GAN生成的图像在亮度统计上与自然图像不同，通常缺少某些饱和或曝光过度的区域。
- **频率统计差异**：Durall等人的研究表明，GAN生成的图像在频率域上存在显著的高频成分，这些成分在自然图像中较少见。

为减少这些差异，文章提出通过调整伪造图像的曝光、模糊和噪声来生成对抗样本，使其统计特征更接近自然图像。

## 4. 统计一致性攻击（StatAttack）
### 4.1 统计敏感的自然退化
文章提出的StatAttack通过以下三种自然退化方法来减少伪造图像与自然图像之间的统计差异：
- **曝光调整**：通过调整像素的局部亮度来改变伪造图像的亮度分布，使其更接近自然图像。
- **高斯模糊**：通过应用高斯低通滤波器，降低伪造图像中的高频成分，使其频率特征与自然图像更加一致。
- **噪声添加**：在伪造图像中添加随机噪声，消除常见的频率伪影，进一步缩小与自然图像的频率差异。

### 4.2 基于分布感知的统计一致性
文章通过最大均值差异（MMD）度量伪造图像和自然图像的特征分布差异。MMD能够量化两类图像特征分布的不同，通过最小化MMD损失来减少伪造图像和自然图像的统计差异，从而生成更难以被检测器识别的对抗样本。

**损失函数**：
损失函数的目标是最小化伪造图像与自然图像之间的分布差异，具体定义如下：
\[
\min_{a, \phi, \sigma, Na} JMMD(P_\theta(\chi_{\text{fake}}), \chi_{\text{real}}) + S(a, \phi)
\]
其中，$JMMD$表示伪造图像和自然图像在检测器中的特征分布差异，$S(a, \phi)$表示曝光平滑约束，$Na$是噪声约束。

## 5. 多层次统计一致性攻击（MStatAttack）
文章提出的多层次StatAttack（MStatAttack）通过在多个层次上应用不同的退化组合（如曝光、模糊和噪声），动态调整每一层的退化权重，以生成更加自然的对抗样本。在优化过程中，软最大值函数保证了组合权重的平衡，避免了图像在多个层次退化时的强度过载。

MStatAttack能够根据每层的组合权重优化攻击策略，进一步提高攻击的成功率和生成对抗样本的自然性。

## 6. 实验
### 6.1 实验设置
文章使用四个数据集（FaceForensics++、StyleGANv2、StarGAN、ProGAN）和六个DeepFake检测器进行实验：
- **空间域检测器**：ResNet50、EfficientNet-b4、DenseNet、MobileNet。
- **频率域检测器**：DCTA、DFTD。

实验通过攻击成功率（ASR）和图像质量（BRISQUE分数）来评估StatAttack和MStatAttack的效果，并与现有的四种基线对抗攻击方法（PGD、FGSM、MIFGSM、VMIFGSM）进行对比。

### 6.2 空间域检测器上的攻击
在白盒设置下，StatAttack和MStatAttack均表现出色。MStatAttack通过优化多层退化组合，提高了攻击成功率。例如，在ResNet和StarGAN数据集上，MStatAttack的攻击成功率从99.3%提升到了100%。

在黑盒设置下，StatAttack和MStatAttack的迁移性显著优于其他方法。通过ResNet生成的对抗样本能够在其他检测器上实现较高的攻击成功率，分别为65.9%、81.3%和71.2%。

### 6.3 频率域检测器上的攻击
在频率域检测器上的实验表明，StatAttack和MStatAttack在白盒和黑盒设置下均有效提高了攻击成功率。最高的攻击成功率可达87.5%。此外，文章的方法能够有效改变伪造图像的频率成分，使其更加接近自然图像。

### 6.4 图像质量比较
通过BRISQUE评分对生成的对抗样本进行质量评估，结果表明，StatAttack和MStatAttack在保持较高攻击成功率的同时，生成了较高质量的对抗样本。MStatAttack生成的图像在视觉上更接近自然图像。

### 6.5 消融实验
通过消融实验，分析了不同退化模式对攻击效果的影响。结果表明：
- **曝光和噪声** 对空间检测

器的攻击效果最大。
- **曝光和模糊** 对频率域检测器的攻击效果显著。

## 7. 结论
文章提出了一种基于自然退化的对抗攻击方法StatAttack，能够通过最小化伪造图像与自然图像的统计差异，有效地绕过多种DeepFake检测器。扩展版本MStatAttack通过多层退化组合进一步提高了攻击的自然性和成功率。实验结果显示，StatAttack和MStatAttack在白盒和黑盒环境下均表现优异，且在不同检测器之间具有良好的迁移性。

### 局限性
文章仅考虑了三种自然退化，未来的研究可探索更多的自然退化方式，进一步增强对抗攻击效果。此外，未来还可通过引入元学习来提升攻击的效率。

### 社会影响
文章提出的攻击方法能够生成逼真的对抗样本，并且具备较强的迁移性，给现有的DeepFake检测器带来了实际的威胁。该方法可用于评估DeepFake检测器的鲁棒性，或通过对抗性训练提高检测器的防御能力。

