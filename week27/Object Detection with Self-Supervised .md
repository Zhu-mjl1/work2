# Object Detection with Self-Supervised Scene Adaptation

## 1. 摘要和引言

### 1.1 摘要

#### 背景
- 本文提出了一种基于**自监督适应**的新方法，用于提升训练好的目标检测器在**固定摄像头场景**下的检测性能。

#### 方法概述
- **伪标注生成**：通过检测器和目标跟踪器在**交叉教学**的方式下生成**伪标签**，用于检测器的自我调整。
- **背景等价性**：在摄像机视角固定的情况下，利用**背景等价性**，提出了一种无伪影的对象混合方法，作为**数据增强**手段。
- **背景提取**：引入了精确的背景提取，作为额外的输入模态，提高检测精度。

#### 数据集与实验
- 提出了一个**大规模且多样化的数据集**，用于开发和评估**场景自适应目标检测**。
- 实验结果表明，所提方法可以显著提高原始检测器的**平均精度（AP）**，并且在多个实验中**超越**了现有的自监督领域适应目标检测方法。

### 1.2 引言
#### 背景与挑战
- **目标检测**在**视频监控**、**自动化零售**等应用中至关重要。
- **现有方法的局限性**：单一检测器的泛化能力有限，难以处理不同场景的复杂性。
- **计算资源与低延迟需求**：实际应用中，许多产品使用**轻量级模型**来实现实时检测，进一步限制了模型的泛化性能。

#### 场景特定检测器
- **提出的解决方案**：与其依赖于单一检测器，我们提出了**场景特定检测器**，可以为每个场景定制检测模型，提高检测效果。
- **轻量化优势**：定制的检测器可以使用轻量级模型，不牺牲检测准确性。

#### 自监督场景适应检测
- **自监督学习方法**：利用未标注的视频帧，通过预训练检测器生成**伪标注数据**，并使用目标跟踪技术扩展检测结果，减少漏检。
- **跨模型教学**：通过多个检测器的**交叉教学**，提升伪标注质量。

#### 贡献
- 提出一个**自监督学习**、**位置感知混合**和**背景建模**相结合的框架，提升固定摄像头场景下的检测性能。
- 提供一个**大规模场景自适应检测数据集**，供后续研究与评估使用。

以下是对“2. Related Work”部分的详细提炼，以适合PPT展示的大纲形式呈现：


## 2. 相关工作

### 2.1 域适应目标检测
- **域适应问题**：即模型在不同域（源域与目标域）之间的表现差异，也称为**域间差距**。场景自适应目标检测是域适应的一个特殊情况。
  - 目标检测器在**全监督源域**训练，并在**目标域**进行适应性调整。
- **研究焦点**：大多数研究集中于**半监督**、**弱监督**或**自监督**适应。
  - 另外，还提出了**无源域数据的自适应方法**，以应对源域数据在适应阶段不可用的情况。

### 2.2 自标注方法
- **自标注方法**：基于**教师-学生模型架构**，教师模型在源域上训练，并在目标域上生成伪标注，学生模型使用这些伪标注进行训练以提升目标域的性能。
  - 处理噪声伪标注的技术：包括**弱/强增强**、**知识蒸馏**、**权重平均**等。
  - **本文方法**：使用**两个教师模型**在源域上训练，并生成聚合和精化后的伪标注，进一步通过**目标跟踪**扩展伪标签（前向和后向跟踪）。

### 2.3 域对齐方法
- **域对齐**：通过强制模型在源域和目标域上输出相似的结果，减少域间差距。域对齐技术通常通过**梯度反转**或**图匹配**来实现。
  - **模型层次**：域对齐可以在图像级别、候选区域级别或实例级别上实现。
  - 这种方法与本文提出的方法互补，可结合使用以提升检测效果。


## 3. 方法

### 3.1 基检测模型在源域上的应用

#### 3.1.1 基模型选择
- 本文选用 Faster R-CNN 作为基础检测器，这是一个精度高且具有灵活模块化结构的目标检测模型，能够适应不同的场景。
  - **模型架构**：Faster R-CNN 包括一个 CNN 主干（在本文中采用 ResNet-50 和 ResNet-101），一个区域生成网络（RPN），以及用于候选区域的分类和边界框回归的 ROI 头。
  - **主干网络选择**：ResNet-50 和 ResNet-101 作为主干网络，具有不同的参数量和性能，本文分别选取它们作为 M1 和 M2 两个基模型。
  - **训练集选择**：在 MSCOCO 数据集上预训练，随后在重新映射的 MSCOCO 数据集上进行微调。映射后的数据集只包含目标域关心的类别（如行人、汽车等），剔除了其他不相关的类别。

#### 3.1.2 迁移到目标域的需求
- **领域迁移问题**：尽管基模型在 MSCOCO 数据集上具有良好的性能，但其泛化能力在新的目标域（如固定摄像头捕捉的场景）中表现不佳。主要问题在于目标域数据分布与源域存在差异。
- **目标域特点**：目标域中的数据来自固定摄像头捕捉的视频流，这些视频中的场景背景是静态的，目标对象（如行人和车辆）是动态的。场景的固定性为背景建模提供了基础，但同时由于摄像头位置固定，物体的姿态和光照条件可能与源域有较大差异。

#### 3.1.3 伪标签生成
- 在目标域没有标注数据的情况下，通过**伪标签**来训练检测器。基检测器可以在未标注的目标域帧上生成部分准确的伪标注。
  - **初步检测**：首先，基检测器会对目标域帧进行检测，输出具有一定置信度的检测框。置信度高于阈值 λdet 的检测框会被保留，作为初步伪标签。
  - **伪标签局限性**：由于源域与目标域之间的分布差异，部分目标可能无法被检测到，或者检测框不准确。因此，后续需要进一步精化这些伪标签。

### 3.2 检测与跟踪生成的伪标签

#### 3.2.1 目标跟踪
- **双向目标跟踪**：为了弥补初步检测的伪标签中可能存在的漏检，本文引入了双向跟踪。通过前向跟踪和后向跟踪两个方向来捕捉物体的移动轨迹，并生成额外的伪标注。
  - **前向跟踪**：在当前帧上检测到的目标通过 DiMP-50 进行前向跟踪，延续多帧，生成一系列连续的伪标签。
  - **后向跟踪**：反向跟踪即从当前帧回溯，生成反向的跟踪结果，确保目标轨迹的完整性。这样能够捕捉到物体运动的更多信息，减少漏检情况，尤其是对距离摄像头逐渐远离或靠近的目标更加有效。

#### 3.2.2 伪标签精化
- **图结构的伪标签精化**：为消除冗余检测框并提高伪标签的准确性，本文设计了一种基于图结构的精化方法。将同一帧中的候选框视为图结构中的节点，利用 IoU 计算候选框之间的相似度，构建边连接相似的框。
  - **去重机制**：通过构建一个图，将每个检测框视为一个节点，如果两个框的 IoU 超过某个阈值λiou，则在这两个节点之间添加一条边。最后，通过删除度数较低的节点（即边连接较少的节点）并保留度数最高的节点，这相当于删除多余的重复检测框，最终保留最具代表性的框作为唯一的有效检测结果
  - **图结构构建过程**：每个检测框代表一个节点，边的权重由框的 IoU 值决定。通过图的连通性分析，移除重复检测框，保留每个目标的最优检测框。

#### 3.2.3 跨模型自监督（Cross-teaching）
- **多模型自监督**：本文采用了两个基模型 M1 和 M2 进行伪标签生成和检测。两个基模型之间通过交叉学习的方式互相监督，互补其各自的弱点。
  - 伪标签由 M1 和 M2 两个基模型生成，经过精化后的伪标签用于进一步训练 M2，从而提高目标域上的适应能力。

### 3.3 位置感知的对象混合（Location-Aware Object Mixup）

#### 3.3.1 Mixup 技术简介
- **Mixup 原理**：Mixup 是一种常用的数据增强方法，通过将两张图像的像素值按比例混合，生成新的样本。这种技术最初在图像分类任务中应用，后来逐渐应用于目标检测任务中。
  - 在目标检测任务中，Mixup 可以在图像级别或实例级别应用，将对象从一张图像“粘贴”到另一张图像上，以增强数据集的多样性。

#### 3.3.2 位置感知的对象混合
- **位置感知对象混合**：相比于传统的随机 Mixup，本方法提出了一种位置感知的对象混合策略。考虑到目标域场景中的背景是静态的，混合过程中需要保持对象与背景的合理位置关系。
  - **随机混合的问题**：传统的随机 Mixup 可能会导致对象在图像中的位置不合理，产生伪影，影响模型性能。
  - **位置感知混合的优势**：通过保持对象在场景中的合理位置，减少伪影的生成，并增强目标检测器对目标域图像的适应性。

#### 3.3.3 混合图像生成流程
选择带有伪标签的目标对象 -> 选择另一随机帧进行对象混合 -> 根据覆盖阈值检查混合检测框 -> 超过阈值则排除原伪标签框


### 3.4 动态背景提取

#### 3.4.1 背景建模
- **背景建模原理**：通过从视频帧序列中移除所有伪标签标注的对象，得到视频的背景图像。由于固定摄像头拍摄的视频背景是静态的，因此可以通过不同帧中的未被遮挡部分来重构背景。
  - 背景提取过程：对于每个帧的像素位置，如果该位置不在任何伪标注框内，则该像素视为背景。通过多个帧的加权平均得到完整的背景模型。
  - 背景变化处理：尽管摄像头固定，但背景可能随着时间推移发生变化（如光照变化），为此本文设计了定期更新背景模型的机制。

#### 3.4.2 背景图像的使用
- **背景掩码**：生成的背景图像与原图像进行融合，以生成对象掩码，用于检测器输入。对象掩码用于突出对象在图像中的位置，帮助检测器更好地区分前景和背景。
- **动态更新**：为保证背景模型的准确性，每隔 Tbg 秒更新一次背景图像，以应对光照等环境变化。

### 3.5 对象掩码融合

- **早期融合**：将输入图像与对象掩码在通道层面进行堆叠，生成一个 6 通道的输入图像（RGB 图像 + 掩码图像），送入 Faster-RCNN 的卷积网络中
  - 能够在卷积特征提取过程中保留对象与背景的区分信息
- **中期融合**：输入图像和对象掩码分别进入特征金字塔网络（FPN），在 FPN 输出的特征图中进行融合。
  - 通过两个独立的 RPN 和 ROI 头处理对象和背景特征，最终在特征金字塔的输出上进行融合。
  - 中期融合可以有效利用多个尺度的特征信息来提升检测性能。
- **晚期融合**：只在 ROI 头部分进行特征融合，减少计算量

- **计算成本与性能权衡**：不同的融合策略引入了不同的计算开销。早期融合计算量较小，但对检测器性能提升有限。中期和晚期融合引入了更多参数和计算量，但能带来更好的检测性能。

## 4. Scenes100 数据集

### 4.1 数据收集

- **关键词搜索**：通过 YouTube 使用关键字 "live view"、"street camera" 和 "live webcam" 搜索固定视角的高质量实时流。
- **录制时间**：从 2020 年 9 月至 2022 年 2 月期间录制，初步记录了 200 个候选视频。
- **数据筛选**：最终选取了 100 个视频，确保每个视频长度超过 2 小时，分辨率不低于 720 像素。
- **场景多样性**：100 个视频来自 16 个国家或地区，涵盖了室内和室外场景，不同时间段和天气条件下录制，保证了数据的丰富性与多样性。
- **视频特征**：每个视频的场景密度和视觉对象的比例差异较大，场景固定，背景静止，适合自监督学习和场景自适应检测。
- **示例帧**：图 5 展示了多种不同场景的标注帧，绿色掩码表示不参与标注和评估的部分。

### 4.2 数据标注

- **标注目标类别**：感兴趣的类别包括**行人**和**车辆**。
  - 行人：包括画面中可见的、可识别的所有人。
  - 车辆：包括所有四轮及以上的车辆（不包括摩托车、自行车和三轮车），对应 MSCOCO 数据集中的汽车、巴士和卡车类别。
- **标注流程**：对于每个视频，前 1.5 小时的部分用于自监督和弱监督训练，剩余部分用于评估。
- **远距离目标处理**：视频中使用广角镜头时，远处的小目标被排除在外，使用多边形掩码手动剔除不可评估区域。
- **标注方法**：先使用预训练检测器检测视觉对象，再由人工细化标注每个帧中的边界框。
  - 统一从视频后半段中采样帧进行标注，帧的数量与视频中目标的数量成反比。
  - 每个视频的标注对象数量基本保持相同。

### 4.3 数据集统计

- **与其他数据集的比较**：Scenes100 与其他目标检测数据集（如 MSCOCO、KITTI、BDD100K、CityScapes）进行比较，表格 1 显示了相关数据。
- **数据量对比**：
  - 场景长度更长：Scenes100 每个视频长达 2 小时，相比之下，CityScapes 每个视频片段仅为 1.8 秒。
  - **场景多样性**：Scenes100 覆盖了多个国家和地区的不同场景，提供了多种天气、道路条件、相机视角以及不同类型的车辆。
  - **固定视角**：Scenes100 中的每个视频均保持固定的相机视角，背景稳定，为动态背景建模提供了条件。
- **数据集特征**：
  - 较长的视频时长为模型提供了足够的数据进行场景自适应。
  - 与现有数据集相比，Scenes100 是第一个专注于**自监督自适应目标检测**的大规模多样化数据集，填补了现有数据集中固定视角长视频数据的空白。
以下是更为详细的 **实验部分**，适合用于PPT大纲：

---

## 5. 实验

### 5.1 评估标准

#### 5.1.1 COCO 评估标准
- 采用 COCO 数据集的标准评估协议计算 **平均精度（AP）** 和 **平均 AP m**，其中 IoU 阈值从 0.5 到 0.95 之间变化。
- 针对特殊场景，对标准 COCO 评估协议进行以下修改：
  - **非评估区域**：对于图像中的小目标或模糊目标，采用掩码排除部分不参与评估的区域，这样避免因小物体或远处模糊物体影响检测性能。
  - **类别权重**：提出一种新的加权 AP 计算方式，**APw**，通过为类别赋予权重解决类不平衡问题，减少数据稀缺类别对平均精度的影响。原始的平均 AP 记为 **APco**。

#### 5.1.2 单视频评估
- 为了评估场景自适应方法的效果，针对每个视频分别计算基础模型和自适应模型的表现，并引入**平均 AP 增益（APG）**，计算自适应后与基础模型的性能差距。
  - **APG 公式**:  
   $$
   APG = \frac{1}{100} \sum_{v=1}^{100} (AP_{v,adapt} - AP_{v,base})
   $$
   - 通过 100 个视频的平均 AP 增益，评估自适应方法的整体有效性。

### 5.2 实现细节


- **检测器**：基于 Detectron2 实现，使用 DiMP 追踪器，未做改动，来自官方 PyTracking 实现。
- **训练集设置**：在训练过程中，来自 MSCOCO 数据集的图像与视频中的未标注帧数量保持一致，用于源域训练和目标域自适应。
- **超参数**：所有视频自适应时，超参数保持一致。训练过程中，所有实验（包括基线方法）都检查了训练损失，确保收敛。
- 对于每个视频单独进行自适应实验
- 训练了“复合模型”，即将所有视频一起进行自适应，进一步验证自适应方法的泛化性

### 5.3 域适应基线

#### 常见的**域适应目标检测**基线方法：

1. **Self-Train (ST)**：
   - 利用检测和单向跟踪生成伪边界框。
   - 训练时为不同的伪边界框赋予不同权重。
   - 没有精细化处理和交叉学习。

2. **STAC**：
   - 基于基础模型生成伪边界框。
   - 在强数据增强后的目标域图像上进行训练。

3. **Adaptive Teacher (AT)**：
   - 动态生成伪边界框。
   - 使用**指数移动平均**更新模型。
   - 结合弱/强数据增强和域分类器进行域对齐。

4. **H2FA R-CNN**：
   - 在 Faster-RCNN 各层次上进行源域与目标域的特征对齐。
   - 使用图像级弱标注（这里伪标注作为弱标注）。

5. **TIA**：
   - 通过域分类器和辅助分类定位头，进行特征对齐。
   - 使用梯度反转层进行训练。

6. **LODS**：
   - 使用风格增强作为数据增强。
   - 使用指数移动平均更新权重。
   - 不使用源域的监督数据。

### 5.4 结果

1. **基础模型性能对比**：
   - **M2** 模型由于拥有更多参数，表示能力更强，因此在 **Scenes100** 数据集上，性能明显优于 **M1**。
   - Scenes100 上的 AP 值已经相对较高，比 **MSCOCO** 的性能仅低约 10 分，因此未观察到大于 5 分的 AP 增益。

2. **适应方法的对比**：
   - 文章提出的最佳方法结合了**伪标签生成**、**位置感知 mixup** 和**对象掩码的中期融合**。
   - 基线方法中，**ST** 和 **LODS** 显著提升了性能，而 **AT** 仅带来边际改进。
   - **STAC**、**H2FA** 和 **TIA** 实际上在 **Scenes100** 上降低了性能。

3. **LODS 方法表现**：
   - **LODS** 方法表现出意外的优异效果，尽管没有使用源域图像，但它在短期内有效。然而，超过 1000 次迭代后，性能急剧下降。

4. **提出方法的优势**：
   - 提出的自我训练方法相比领域对齐方法表现更加稳健和一致，展现了**更高且稳定的性能改进**。
   - 领域对齐方法由于**对抗性学习**目标不稳定，优化难度大，因此表现出更强的超参数和训练计划敏感性。

5. **Scenes100 的独特性**：
   - 与传统领域自适应数据集相比，**Scenes100** 数据集场景间差异大，且域内变化较少，因此需要不同的设置。
   - 提出的方法在此环境中表现出**鲁棒性**，无需频繁调整超参数，使用相同的设置对所有 100 个视频进行训练。

### 5.5 消融实验

1. **伪标签、Mixup 和对象掩码融合的影响**：
   - 实验保留**伪标签生成**不变，因为它是其他组件的基础。
   - **伪标签**单独应用时，已经能带来显著的 AP 增益，甚至优于更复杂的 ST 方法（ST 方法包含伪标签框加权）。
   - **位置感知的 Mixup** 比随机 Mixup 表现更好，验证了减少伪影对模型适应性能的积极影响。

2. **对象掩码融合方法对比**：
   - 对象掩码的三种融合方式（早期、中期、晚期融合）进行对比，结果显示**中期融合**效果最佳。
   - **中期融合**中，RPN 利用融合后的特征金字塔，该金字塔包含了关键的对象边界信息，因此表现优于早期和晚期融合。

3. **Mixup 和对象掩码融合的结合效果**：
   - Mixup 和对象掩码融合的结合进一步提升了性能，证明了两者协同作用对目标检测效果的显著提升。

4. **特征金字塔融合方法对比**：
   - 对特征金字塔的融合方法进行进一步实验，比较了平均池化、CNN 和注意力模块。
   - 结果显示，**平均池化**的表现略优于使用 CNN 或注意力模块，可能是因为新引入的模块（如 CNN 和注意力模块）需要额外的监督训练数据，而其权重在初始化时是随机的。

5. **实验结果总结**：
   - 伪标签生成、位置感知 Mixup 和对象掩码中期融合的组合，能够显著提升适应性能，在不同的实验组合中表现最佳。
   - 实验还表明，增加复杂模型（如 CNN 和注意力模块）未必能带来性能的提升，反而可能因为需要额外的训练数据导致效果略微下降。
   
   
### 6. 总结（分点）

1. **提出了一种自监督的场景自适应目标检测框架**：
   - 通过基础检测器和通用跟踪器生成伪目标标签，并在**交叉教学**的方式下作为自适应训练的目标。
   
2. **充分利用固定摄像头视角下的背景等变性**：
   - 提出了一种无伪影的**位置感知对象混合**，用于增强输入图像。
   - 使用**动态背景提取**作为检测器的额外输入模态，进一步提升检测效果。

3. **引入首个大规模场景自适应目标检测数据集：Scenes100**：
   - 与其他领域自适应目标检测数据集相比，Scenes100 具有多种独特的特性，涵盖了固定摄像头视角下的场景。

4. **实验结果显著**：
   - 所提出的方法在 Scenes100 数据集上的表现大幅优于领域自适应目标检测基线方法。
   
5. **消融实验验证了框架中各组件的有效性**：
   - 通过实验，展示了伪标签生成、对象混合和动态背景提取等组件在框架中的重要性和效果。